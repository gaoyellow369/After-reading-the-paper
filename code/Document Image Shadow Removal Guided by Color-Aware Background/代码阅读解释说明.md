在`extractBackground.py`中通过高斯混合模型（GMM）和双边滤波提取文档的背景，用于作为后续后续的 `CBENet` 和 `BGShadowNet` 训练的Ground Truth。

使用 GMM 对图像分块，提取颜色均匀的背景区域。

```python
#gmm = mixture.GaussianMixture(n_components=2, covariance_type='full')
#gmm.fit(patch)
#cls = gmm.predict(patch.flatten().reshape(patch_h*patch_w, patch_c))
patch = x[sub_h:sub_h_end, sub_w:sub_w_end, :]
gmm = mixture.GaussianMixture(n_components=2, covariance_type='full')
gmm.fit(patch)
cls = gmm.predict(patch.flatten().reshape(patch_h * patch_w, patch_c))
cls0_colors = patch[cls == 0]
cls1_colors = patch[cls == 1]
cls0_avg_color = get_average_color(cls0_colors)
cls1_avg_color = get_average_color(cls1_colors)
if np.sum(cls0_avg_color) >= np.sum(cls1_avg_color) or np.isnan(cls1_avg_color).any():
    background_img[sub_h:sub_h_end, sub_w:sub_w_end, :] = cls0_avg_color
else:
    background_img[sub_h:sub_h_end, sub_w:sub_w_end, :] = cls1_avg_color

```

使用双边滤波平滑背景，减少噪声

```python
#bilater_image_2 = Bilater(I1, background_img, gsigma, ssigma, winsize)
def Bilater(image, I_2, gsigma, ssigma, winsize):
    gkernel = gaus_kernel(winsize, gsigma)
    kernel = np.zeros((winsize, winsize))
    for i in range(r, row - r):
        for j in range(c, col - c):
            skernel = np.exp(-np.power((image[i, j] - image[i - r:i + r + 1, j - c:j + c + 1]), 2) / sigma2)
            kernel = skernel * gkernel
            kernel = kernel / sum(sum(kernel))
            for channel in range(3):
                bilater_image_2[i][j][channel] = np.sum(I_2[i - r:i + r + 1, j - c:j + c + 1, channel] * kernel)
    return bilater_image_2[r:-r, c:-c, :]
```

颜色感知背景估计网络 `CBENet`主要在`CBENet.py`, `layers.py`中进行定义，用于生成背景图，指导影子移除

`CBENet.py`中的下采样

```python
for i in range(len(self.down_blocks)):
    out = self.denseBlocksDown[i](out)  # DenseBlock 提取特征
    skip_connections.append(out)        # 保存跳跃连接特征
    out = self.transDownBlocks[i](out)  # TransitionDown 降低分辨率

```

`CBENet.py`中的上采样

```python
for i in range(len(self.up_blocks)):
    skip = skip_connections.pop()       # 获取跳跃连接特征
    out = self.transUpBlocks[i](out, skip)  # TransitionUp 恢复分辨率并融合特征
    out = self.denseBlocksUp[i](out)    # DenseBlock 重建细节

    
out = self.finalConv(out)
return out    
```



`BGShadowNet`有两个阶段，分别在`stageI.py`和`stageII.py`中定义了网络结构

`stageI.py`通过利用 DenseNet 对输入影子图像生成粗略去影结果。

定义网络结构

```python
def BGShadowNet1(in_channels=3):
    return FCDenseNet(
        in_channels=in_channels, down_blocks=(4, 4, 4, 4, 4),
        up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=16,
        growth_rate=12, out_chans_first_conv=48)

```

特征提取与生成粗略图像

```python
for i in range(len(self.down_blocks)):
    out = self.denseBlocksDown[i](out)  # 下采样提取特征
    skip_connections.append(out)
    out = self.transDownBlocks[i](out)
for i in range(len(self.up_blocks)):
    skip = skip_connections.pop()
    out = self.transUpBlocks[i](out, skip)
    out = self.denseBlocksUp[i](out)    # 上采样重建图像

```

`BGShadowNet1` 的输出包括去影图像 `confuse_result` 和特征图 `newFeatureMap`

```python
def forward(self, x, featureMaps):
    ...
    return out, newFeatureMap
```







`stageII.py`通过背景融合与特征增强，细化影子移除结果

`BAModule`每层包括一个卷积模块和残差模块，用于计算特征的注意力权重。每个注意力模块都与下采样阶段的特定层对应（`att0` ~ `att4`）

```python
self.att0 = nn.Sequential(
    ConvBlock(96 * 2, 96 * 2, 3, 1, 1, sn=True),  # 卷积提取特征
    ResidualBlock(96 * 2, 96 * 2, activ='sigmoid', sn=True)  # 注意力权重计算
)
```

`BAModule`计算注意力权重并融合特征，将输入特征 `out` 和对应背景特征 `background_featuremap` 连接，计算注意力权重 `att`。

```python
skip = torch.cat((out, background_featuremap), 1)  # 融合特征和背景
att = getattr(self, 'att{}'.format(i))(skip)  # 计算注意力权重
skip = skip * att  # 应用注意力权重调整特征
skip_connections.append(skip)
```

`DEModule` 通过三个阶段增强输入特征：

- 起始阶段：提取特征。
- 增强阶段（`Texture Enhance Module, TEM`）：增强多分辨率特征。
- 最终阶段：融合并降维

```python
class DEModule(nn.Module):
    def __init__(self, in_channel):
        self.conv_start = ConvBNReLU(in_channel, 256, 1, 1, 0)
        self.tem = TEM(128)  # 调用多分辨率纹理增强模块
        self.conv_end = ConvBNReLU(512, 192, 1, 1, 0)
```

`DEModule`前向过程如下

```python
def forward(self, x):
    x = self.conv_start(x)
    x_tem = self.tem(x)  # TEM 提取纹理特征
    x = torch.cat([x_tem, x], dim=1)
    x = self.conv_end(x)  # 合并后进行降维
    return x
```

`TEM `使用多分辨率分组操作对特征进行增强，生成高质量细节

```python
class TEM(nn.Module):
    def forward(self, x):
        sta, quant = self.qco(x)  # 量化增强
        ...
        return out
```

生成细化结果

```python
def forward(self, confuse_result, background, shadow_img, featureMaps):
    for i in range(len(self.down_blocks)):
        out = self.denseBlocksDown[i](out)
        background_featuremap = background_feature[i]
        skip = torch.cat((out, background_featuremap), 1)  # 融合背景与下采样特征
        out = self.transDownBlocks[i](out)
    ...
    out = self.bottleneck(out)  # 进一步提取高层特征
    for i in range(len(self.up_blocks)):
        out = self.transUpBlocks[i](out, skip, featureMap)  # 特征融合
        out = self.denseBlocksUp[i](out)
    out = self.finalConv(out)  # 生成最终图像
    return out
```

在 Stage2 中，`confuse_result` 和 `newFeatureMap` 被作为输入，与背景图结合

```python
def forward(self, confuse_result, background, shadow_img, featureMaps):
    x = torch.cat([confuse_result, shadow_img], dim=1)  # 融合输入影像与粗略结果
    ...
    out = self.transUpBlocks[i](out, skip, featureMap)  # 上采样融合
    return out
```

在 Stage2 中，`BAModule`融合背景与多分辨率特征

```python
skip = torch.cat((out, background_featuremap), 1)  # 背景与特征融合
att = getattr(self, 'att{}'.format(i))(skip)  # 计算注意力权重
skip = skip * att  # 应用注意力权重
```

在 Stage2 中，将 `DEModule` 的输出加入跳跃连接特征，增强全局特征

```python
DEModuleinput = torch.cat([DEModuleFirst, skip_connections[1]], dim=1)
DEModuleresult = self.DEModule(DEModuleinput)
skip_connections[1] = torch.cat([skip_connections[1], DEModuleresult], dim=1)
```





最后的判别器`Discriminator`优化主要在`models.py`中

```python
class Discriminator(nn.Module):
    def forward(self, input):
        x0 = self.Cv0(input)
        x1 = self.Cv1(x0)
        ...
        out = self.Cv4(x3)  # 判别生成图像的真实性
        return out

```

损失函数定义在`helper_BGShadowNet.py`, `VGG_loss.py`中