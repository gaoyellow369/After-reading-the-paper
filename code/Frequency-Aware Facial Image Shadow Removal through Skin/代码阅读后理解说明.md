ps. 原始GitHub代码没有md说明介绍，也几乎没有代码注释...... 因此本说明均基于个人阅读后的理解，如有误差还请指正

这里对于图像预处理就不多加赘述了，直接上核心代码
定义网络的核心代码主要是两个，一个是FSRNet.py，一个是help.py

help.py中定义了FSABlock，CFModule，TFModule这三个最为关键的子模块，经过验证，结构符合论文中的图解，对应每次卷积池化归一化等操作的细节均能对应上论文

```python
class FSABlock(nn.Module):
    def __init__(self, dim, dim_out, *, norm_groups=32, dropout=0, with_attn=False):
        super(FSABlock, self).__init__()
        self.with_attn = with_attn  # 是否启用自注意力机制

        # 残差块（ResNet Block）
        self.res_block = ResnetBlock(
            dim, dim_out, norm_groups=norm_groups, dropout=dropout
        )

        # 自注意力模块（Transformer Block）
        if with_attn:
            self.attn = TransformerBlock(
                dim_out, norm_groups=norm_groups, ffn_expansion_factor=2.66, bias=False
            )

    def forward(self, x):
        # 首先通过 ResNet 残差块
        x = self.res_block(x)

        # 如果启用了自注意力，则进一步通过 Transformer 块
        if self.with_attn:
            x = self.attn(x)

        return x
```



```python
class TFModule(nn.Module):
    def __init__(self, pre_channel, ffn_expansion_factor, bias):
        super(TFModule, self).__init__()    
    
        self.norm = LayerNorm(pre_channel)  # 层归一化，用于标准化输入特征
        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # 全局平均池化，用于提取全局信息

        # 局部特征嵌入 (Patch Embedding)
        self.xpatchembedding = nn.Conv2d(pre_channel, pre_channel, kernel_size=3, stride=1, padding=1, groups=pre_channel, bias=bias)
        self.featurepatchembedding = nn.Conv2d(pre_channel, pre_channel, kernel_size=3, stride=1, padding=1, groups=pre_channel, bias=bias)

        # 两层非线性变换 (MLP)
        self.xline1 = nn.Conv2d(pre_channel, pre_channel // ffn_expansion_factor, 1, bias=False)
        self.xline2 = nn.Conv2d(pre_channel // ffn_expansion_factor, pre_channel, 1, bias=False)
        self.faceline1 = nn.Conv2d(pre_channel, pre_channel // ffn_expansion_factor, 1, bias=False)
        self.faceline2 = nn.Conv2d(pre_channel // ffn_expansion_factor, pre_channel, 1, bias=False)

        # 输出层 (融合后的特征)
        self.project_out = nn.Conv2d(pre_channel * 2, pre_channel, kernel_size=1, bias=bias)

    def forward(self, x, feature):
        x_1 = self.xpatchembedding(self.norm(x))  # 对输入特征进行归一化和嵌入
        feature_1 = self.featurepatchembedding(self.norm(feature))  # 对辅助特征进行归一化和嵌入

        # 全局池化
        x_1 = self.avg_pool(x_1)
        feature_1 = self.avg_pool(feature_1)

        # 非线性变换
        x_1 = torch.sigmoid(self.xline2(F.relu(self.xline1(x_1))))
        feature_1 = torch.sigmoid(self.faceline2(F.relu(self.faceline1(feature_1))))

        # 特征加权
        new_x = feature * x_1.expand_as(x)
        feature = x * feature_1.expand_as(feature)

        # 特征融合
        g = torch.cat([new_x, feature], dim=1)  # 通道拼接融合
        return self.project_out(g)  # 输出融合后的特征
```



```python
class CFModule(nn.Module):
    def __init__(self, pre_channel, ffn_expansion_factor, bias, alpha=1.0):
        super(CFModule, self).__init__()    
    
        self.alpha = alpha
        self.norm = LayerNorm(pre_channel)  # 层归一化
        self.project_in = nn.Conv2d(pre_channel, pre_channel * 2, kernel_size=1, bias=bias)

        # 多尺度卷积操作
        self.x3x3 = nn.Conv2d(pre_channel * 2, pre_channel * 2, kernel_size=3, stride=1, padding=1, groups=pre_channel * 2, bias=bias)
        self.face3x3 = nn.Conv2d(pre_channel * 2, pre_channel * 2, kernel_size=3, stride=1, padding=1, groups=pre_channel * 2, bias=bias)

        # 输出层
        self.project_out = nn.Conv2d(pre_channel * 2, pre_channel, kernel_size=1, bias=bias)

    def forward(self, x, feature):
        # 投影到高维空间
        h = self.project_in(self.norm(x))
        feature = self.project_in(self.norm(feature))

        # 多尺度卷积操作
        x1_3, x2_3 = F.relu(self.x3x3(h)).chunk(2, dim=1)
        face1_3, face2_3 = F.relu(self.face3x3(feature)).chunk(2, dim=1)

        # 通过 AdaIN 进行特征对齐
        t1 = adain(x1_3, face1_3)
        t2 = adain(x2_3, face2_3)

        # 线性插值
        t1 = self.alpha * t1 + (1 - self.alpha) * x1_3
        t2 = self.alpha * t2 + (1 - self.alpha) * x2_3

        # 特征融合
        h_feature = F.relu(self.x3x3(t1))
        face_feature = F.relu(self.face3x3(t2))
        g = torch.cat([h_feature, face_feature], dim=1)

        return self.project_out(g + x)  # 返回融合后的特征
```



FSRNet.py主要定义了整个论文的主要处理流程

其中Frequency-domain Encoder如下

```python
downs = [nn.Conv2d(in_channel, inner_channel, kernel_size=3, padding=1)]  # Conv3x3 初始卷积
for ind in range(num_mults):
    is_last = (ind == num_mults - 1)
    use_attn = (str(now_res) in str(attn_res))  # 控制是否使用注意力机制
    channel_mult = inner_channel * channel_mults[ind]
    downs.append(FSABlock(pre_channel, channel_mult, norm_groups=norm_groups, with_attn=use_attn))
    pre_channel = channel_mult
    if not is_last:
        downs.append(Downsample(pre_channel))  # 下采样
        now_res = now_res // 2
self.downs = nn.ModuleList(downs)
```

其中Frequency-domain Decoder如下

```python
ups = []
for ind in reversed(range(num_mults)):
    is_last = (ind < 1)
    use_attn = (str(now_res) in str(attn_res))  # 是否启用注意力机制
    channel_mult = inner_channel * channel_mults[ind]

    # 上采样 + FSABlock
    for _ in range(0, res_blocks + 1):
        ups.append(FSABlock(
            pre_channel + feat_channels.pop(), channel_mult,
            norm_groups=norm_groups, dropout=dropout, with_attn=use_attn
        ))
        pre_channel = channel_mult

    # 最后不是最高分辨率的层，加入上采样模块
    if not is_last:
        ups.append(Upsample(pre_channel))
        now_res = now_res * 2

self.ups = nn.ModuleList(ups)

```

TFModule和CFModule的输出和原始图片经过Frequency-domain Encoder后的输出进行在Generator中按照特征大小依次合并并上采样操作得到整个模型最终输出的融合过程如下

```python
class Generator(nn.Module):
    def forward(self, x, facefeaturemaps, featuremaps):
        inp = x  # 原始图片
        feats = []

        # 编码阶段：提取多分辨率特征
        for layer in self.downs:
            x = layer(x)
            feats.append(x)

        # 解码阶段：逐步融合特征
        for layer in self.mid:
            if isinstance(layer, CFModule):
                # CFModule 融合颜色特征
                facefeaturemap = facefeaturemaps.pop()
                x = layer(x, facefeaturemap)
            elif isinstance(layer, TFModule):
                # TFModule 融合纹理特征
                featuremap = featuremaps.pop()
                x = layer(x, featuremap)
            else:
                # 其他模块的处理
                x = layer(x)

        # 上采样阶段，融合 Encoder 提取的多尺度特征
        for layer in self.ups:
            if isinstance(layer, FSABlock):
                feat = feats.pop()  # 获取编码器特征
                x = layer(torch.cat((x, feat), dim=1))  # 拼接解码器与编码器特征
            else:
                x = layer(x)

        # 最终输出
        return x + inp

```

